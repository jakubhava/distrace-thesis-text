\chapter{Introduction}
Lately, the volume of data applications need to handle is significantly increasing. In order to support this scaling trend, the applications are becoming distributed for reasons of scalability, stability and availability. Not every task may be solved efficiently by distributed applications, however when it comes to big data, the computational requirements may be higher than single physical node can fulfill.  Such distributed applications may run on multiple physical or virtual machines in order to achieve the best performance and the ability to process data significantly large. For this, computation clusters are created where the user interacts with the application as it would be running locally and the cluster should handle the distributed computation internally.

However, with growth of distributed applications there is also increasing demand for monitoring or debugging such applications. Analyzing applications in distributed environment is inherently more complex task comparing to single-node applications where well-known debugging or profiling techniques may be used. Analysis of single-node applications usually focus on a single standalone application where most of the information required to reason about it is collected directly from the application. In case of distributed application it is desired to collect the same information as on single-node applications plus, and more importantly, the state between the communicating nodes. For instance, an error may occur on one of the computation nodes in the cluster and over time more and more nodes are becoming affected. By collecting the relations between the nodes the analysis tool may be able to to use the information to where the error initially occurred and how it spread over the time. 

Simple solution comes to mind to address this issue. Monitoring or debugging tools used for single-node applications may be attached per each application node and collect the information from the nodes separately on each other. This solution does not require any additional tools however the state between the application nodes would not be preserved unless the monitored application is already designed to send the required information. Most of the applications is not designed to transfer the information used for analysis of the application itself for several reasons. It may be hard or unwanted to design the application in a way that all the information required for analysis are already transferred between communication nodes. New analysis method may require new metrics which in this case would also mean recompiling and new deployment of the application.

For this reason several new monitoring and debugging tools have been developed. These tools are usually build on the code instrumentation technique. This method is used to alter the monitored application's code at run-time in order to collect all relevant information. The significant advantage of this method is that the original application does not have to be changed in order to add additional metrics. Usually such tools use the instrumentation technique to add special information to the code which is later used to build a so-called distributed stack-trace.  Stack-trace in single node application represents the call hierarchy of method at the given moment. Distributed stack-trace is a very similar concept except that the dependencies between different nodes are preserved and can be seen on the collected stack-trace as well. Therefore, distributed stack-traces allows us to see the desired relations between the applications node. Google Dapper and OpenZipkin are the most significant available monitoring tools and are discussed later in the thesis. 


\section{Project Goals}
This thesis introduces monitoring tool for the similar purposes, sharing some of the concepts mentioned above, however the goals of this work are be different and should give the user a new generic and high-performance way how to monitor the applications.  Main goals of the thesis are to create an open-source generic monitoring library with small footprint on the monitored applications whilst giving the user the possibility to use high-level programming language to define their instrumentation points.

Main requirements for the platforms are:
\begin{itemize}
	\item \textbf{Small Footprint} \newline
	 The mentioned cluster monitoring tools can affect the application performance and memory consumption since they perform instrumentation in the same virtual machine as the monitored application. The project is required to have a minimal footprint on the monitored application. The instrumentation is needed in order to inject special information about spans to the application's code. This information is later used for the span collection and associating the relationships between spans.
	\item \textbf{Application-level Transparency and Universality} \newline
	These two requirements are contradictory. The universal tool which could be used for monitoring majority of applications would either collect just basic information shared about all applications or the user would be required to manually specify the information specific to the application which leads to the loose of the application-level transparency. This tool tries to find compromise between these two goals and support high-level of universality with minimizing the impact on the application itself.
	The project needs to do some trade-offs between the application-level transparency and the universality of the platform. The goal is be able to instrument JVM-based applications to minimizing the impact on the application's code itself.
	\item \textbf{Easiness of Use} \newline
	The application should use high-level programming language for the instrumentation and specifying additional information to be collected. The users of this tool are supposed to work with Java-based language and should not be required to have deeper knowledge about internal Java Virtual Machine structure.
	\item \textbf{Easiness of Deployment} \newline
	The complexity of deployment of this tool is also the significant aspect of the tool. In order for developers and testers to use this tool frequently, its deployment and usage has to be relatively easy. This requirement has two sub-parts. Minimizing the configuration of the monitoring tool to the bare minimum and also minimizing the number of artifacts the users of this tool are required to use.
	\item \textbf{Modularity} \newline
	The thesis should be designed in a way that some parts of the whole tool may be substituted by user specific modules. For example, the users should be allowed to switch the default user interface to the the user interface they prefer without significantly changed the code of the tool.
\end{itemize}

The discussion of different approaches for meeting the requirements above are discusses in the following section.

\section{Thesis Outline}
The thesis starts with the Background chapter. The purpose of this chapter is to give the reader overview of relevant tools to the thesis  such as overview of several profiling tools, instrumentation and communication libraries. It also describes the relevant cluster monitoring tools like Google Dapper and OpenZipkin in more detail. This chapter ends with the Analysis section containing discussion of how specific requirements of the thesis are met. I also mentions the weaknesses of the other cluster monitoring tools and describes how the thesis tries to overcome them. The following Design section starts with an Overview section. This section depicts the architecture of the whole system. Further the Design chapter contains sections for each important parts of the application. It contains 

