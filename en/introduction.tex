\chapter{Introduction}
The volume of data processed is becoming larger every day. In order to process this data, the application are becoming distributed for reasons of scalability, stability and availability. However having these applications available is not sufficient. We need to be able to monitor and reason about distributed applications as well since only then we can efficiently find bugs and improve performance of such applications. However, reasoning about distributed applications is inherently more complex compared to single-node applications. In case of single node application we can use standard monitoring or profiling tools producing output we can use to reason about the application, but in the complex cluster solutions this can't be applied. Standard monitoring tools per each node in the cluster may be used however we still could reason only about one particular node and not the whole cluster.

Several monitoring tools for such a problem have been already developed. The most significant one is proprietorial software from Google called Google Dapper and open-sourced tool called Zipkin. They both build on a simple idea called distributed trace. In single node applications we can get current stracktrace which represents the call hierarchy at given moment. Distributed stack trace is the very same concept except that the dependencies between different nodes are preserved in that stack too. Therefore, using the distributed traces we can reason about more complex systems.

This thesis introduces another monitoring tool for the similar purpose, however it has some different concepts than the tools mentioned above. Google Dapper is build proprietorial and available only for Google applications. In case of Zipkin, the user has to instrument the application itself by introducing the annotation. This thesis tries to overcome these issues by creating open-source monitoring platform with small footprint on the monitored applications whilst giving the user the possibility to use high-level programming language to define their instrumentation points.

\section{Project Goals}
The project makes trade-offs between application level transparency and easiness of use. It is not an universal monitoring tool which could be used out of the bug but it can be thought of as an extendable library providing the developer with means how to instrument their specific application in high-level programming language such as Java. All instrumentation specific internals and low-level code is hidden from the user but low-level overhead is still achieved by multiple techniques discussed later. In order to use this platform on some particular application, the programmer has to extend the prepared library for the application by defining points where instrumentation should take place, but the original application's code remains unaffected and thus no recompilation of classes is required.

The project should also be extensible in a way that information from additional low-level system monitoring tools can be attached to the monitored data - such as the memory usage or data allocation. We use native java agent written in C++ for the instrumentation purposes and the architecture is prepared to combine the monitored data from our tool with the other external tools in the future.  

Using the native client we achieve low-overhead on the system and can query various interesting information such as number of loaded classes, garbage collection time and so on. To minimize performance and memory effects on the monitored applications, the instrumentation does not happen in the same JVM as the monitored applications runs. The native agents informs secondary helper JVM with our instrumentor running in it about the loaded classes and the instrumentor JVM decides whether the class should be instrumented or not and instruments the classes when required. This instrumentor JVM can also be shared by all the nodes in the cluster which has yet another advantage hat once any class has been instrumented by any node in the cluster, the other nodes just obtain the instrumented bytecode without the delays for instrumentation itself.

The easiness of deployment is also the significant aspect of the introduced tool. In order for developers and testers to use this tool, it needs to be relatively easy to deploy it and use it. This was achieved by limiting the number of artifacts to the bare minimum and therefore when it comes to using the tool, the user has only two files - native agent which needs to be attached to monitored application and the instrumentation server written in java which handles the instrumentation for the whole cluster application. Several deployment strategies exist and are discussed later in the \ref{chap:evaluation} chapter.

\section{Thesis outline}
The thesis starts 

