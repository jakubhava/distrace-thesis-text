\chapter{Realistic Case Study}
\label{chap:big_example}
This chapter demonstrates how the Distrace tool can be used on a bigger example and also serves as the user manual for creating custom monitoring applications. We will go through all the steps from creating the application up to running the application and accessing the observed results.

This example is based on the H\textsubscript{2}O\footnote{More information about H\textsubscript{2}O can be found at \url{https://www.h2o.ai} and \url{https://github.com/h2oai}.} open-source fast scalable machine learning platform. This platform supports various methods for building machine learning models, methods such as deep learning, gradient boosting or random forests. The core of the H\textsubscript{2}O platform is written in Java and clients for different languages exist as well. Internally, H\textsubscript{2}O is using map-reduce computation paradigm\footnote{Map-reduce is a programming model in distributed systems. The basic idea is to split tasks into smaller parts and perform the map operations. The intermediate results are then combined together using the reduce calls until the complete result has been assembled from all the sub-tasks.} to perform various tasks across the cluster.

The goal of this example is to monitor a subset of map-reduce tasks and see visualizations of the computation process. This can help reasoning about the performance of the platform and can discover unwanted delays in the computations. This chapter first describes relevant parts of the H\textsubscript{2}O platform in more details. In the following sections, we describe in steps how to extend the core instrumentation library for H\textsubscript{2}O purposes. Lastly, we show how this example can be started and how visual outputs can be interpreted. 
This full example is also available in the attached source code of the thesis in the Attachment 1. More examples are available and their complete list together with the instructions on how to run them are in the Attachment 2.

\section{H\textsubscript{2}O In More Details}
H\textsubscript{2}O is in-memory machine learning platform. The platform is distributed and the computations are performed in the cluster. All nodes in the cluster are equal and each of them can initialize the computation. Most computations are performed as map-reduce tasks. This section describes the internal format of data used in the H\textsubscript{2}O platform, how data are stored on the nodes and lastly, how the computations are performed in the cluster. 

\subsection{Data in H\textsubscript{2}O}
Data is stored in H\textsubscript{2}O in so-called \texttt{Frames}. A \texttt{Frame} is column based distributed data table with columns and rows. The \texttt{Frame} is designed in a way, that it can handle data too big to fit into a memory of a single machine. Each column is represented by the \texttt{Vec} class. This class represents a vector of data that is again distributed across the nodes in the cluster. Further, each vector is split into multiple \texttt{Chunks} and each chunk represents the part of the vector physically available on a single node. Therefore, chunks are units of parallelization and distribution.

Hence, it is possible for one node to contain multiple chunks from a single vector and therefore, the number of chunks in the vector does not represent the number of nodes on which the data is distributed. Typically, data imported to H\textsubscript{2}O is distributed via chunks equally among all the nodes in the cluster, but algorithm may also decide to distribute the data on just several nodes in the cluster. It is also possible to create the frame manually and specify on which nodes the chunks will be stored. Therefore, the frame may be distributed only on a portion of the cluster. Usually, when chunks are being created on some specific node, chunks of the same size for each column are created on that node. This means that each node storing the data usually have corresponding (neighbor) chunks for all the columns. This can be thought of as that each node storing some data has a subset of rows from the full table with all columns. The point of this paragraph is that the distribution of chunks controls the parallelization.

The Figure \ref{fig:h2o_frame} shows the structure of a frame with three columns, where each column is split into two chunks. It also shows how chunks may be distributed in the cluster of size three. We can also see in the figure that corresponding chunks for each vector have the same size and are stored on the same nodes. Also, it is important to mention that data in H\textsubscript{2}O is not replicated.

	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{h2o_frame.png}
		\caption{Structure of the H\textsubscript{2}O frame and its distribution in the cluster.}
		\label{fig:h2o_frame}
	\end{figure}

\subsection{Computation in H\textsubscript{2}O}
When the user tells H\textsubscript{2}O to create a deep learning model based on the data on the input, H2O sees this as a \textbf{Job}. Jobs are used for tracking long-lifetime user interactions and encapsulate the whole computation from the user point of view. The job encapsulates computation tasks of several map-reduce tasks. In H2O, the class \texttt{MRTask} is used as the core implementation of a map-reduce task\footnote{Different type of tasks exist in H\textsubscript{2}O, however only \texttt{MRTasks} are mentioned here since they are important to this example.}. The map-reduce task is always bound to some H\textsubscript{2}O frame on which the computation needs to be performed. This class is used to encapsulate the task, partition it to smaller tasks and run remote computations among the whole cluster. 

Each task contains the information about on which H\textsubscript{2}O nodes the task itself and its sub-tasks can be computed. When a task is split, this range is divided and each sub-task can be computed on the half of the nodes the parent tasks specifies. The task is executed on each H\textsubscript{2}O node which contains chunks required for this task. The \texttt{map} operations are called on each node, which contains locally available chunks for the frame this task is based on. The \texttt{reduce} calls are used to reduce the results from two sub-tasks into a new task combining results from the children.

In more details, the \texttt{MRTask} class extends from the \texttt{DTask} class. This class is a general class used in H\textsubscript{2}O to represent a task remotely executed. Further, the \texttt{DTask} class extends from the \texttt{H2OCountedCompleter} class. The last mentioned class is a simple wrapper around the Fork/Join (F/J) execution framework\footnote{More information about the Fork/Join framework can be found at the Java documentation available at \url{https://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html}.} allowing the platform to prioritize tasks. The Fork/Join framework is an implementation of Java \texttt{ExecutorService}, which helps with job parallelization on multiple processors. The Fork/Join framework executes tasks in separated threads and can move tasks between threads to ensure the highest possible performance. Each H\textsubscript{2}O \texttt{MRTask} is executed as \texttt{ForkJoinTask} inside this execution framework. A \texttt{ForkJoinTask} is a task wrapper, which is executed in a single thread. It is a light-weight wrapper and a large number of tasks may be served by a smaller number of actual threads.

The way how H\textsubscript{2}O perform computations from the high-level point of view can be seen in Figure \ref{fig:h2o_overview}. The task initiator receives the \texttt{MRTask} from the parent \texttt{Job} or from the user. It splits the task into two new sub-tasks and sends these tasks to up to two different nodes in the cluster. These nodes again split the task from the input and send the task to another selected nodes in the cluster. Each splitting decreases the number of nodes on which the task has to be sent. Once this number is equal to one, the node is marked as a leaf node and does not perform any other splitting and re-sending.

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{h2o_overview.png}
	\caption{High-level overview of execution hierarchy.}
	\label{fig:h2o_overview}
\end{figure}

It is important to say that the \texttt{MRTask} is always distributed to all nodes in the cluster. This figure shows how it is ensured that each node participates in the computation, however, the computation step itself is still missing. Each node of the cluster, who receives a task, also submits the task for computation into the Fork/Join execution framework. This computation performs the mapping operation on all the chunks, which are available locally on the node executing this task, for the frame associated to the task. The \texttt{reduce} operation follows the \texttt{map} operation, however, we need to first ensure that the child tasks, from which we want to combine the results together, are already finished. This is ensured by child tasks signaling to the parent tasks when the work has been finished. Therefore, parent tasks are informed when they can start reducing the results. 

The computation on a single node is shown in the following pseudo-code:
\begin{lstlisting}[language=Java]
public void singleNodeComputation(){
  MRTask task = ... // task received from the parent or from the user
  MRTask left = split(task, start1, end1)
  MRTask right = split(task, start2, end2)
  remoteCompute(left)
  remoteCompute(right)
  H2O.submitTask(task) // submit this task into local F/J
  ..
  // task is taken from F/J for execution
  task.map(..)
  task.waitForComplete() // wait for the child tasks to finish
  task.reduce(left, right)
  notifyComplete(task) // notify the parent of completion
}
\end{lstlisting}
The split method accepts indices representing the nodes in the cluster on which this sub-task and further sub-sub-tasks may be executed.

The last missing piece of information is a description of what is done when the node has more chunks available for the frame associated with the task since each \texttt{map} operation is executed only per single chunk. In case the node has multiple chunks available for the frame, the node always locally submits two new tasks into the F/J framework, each having half of the chunks. This is recursively repeated until we have single chunk tasks, which are processed normally. These locally-split tasks inform their parents that they are done. This signalization goes up the tree until the original task is marked finished.

\subsection{Methods for Instrumentation}
For the purpose of this example, a special \texttt{MRTask} called \texttt{SumMRTask} has been created. This task just performs distributed sum of range of numbers. In order to be able to visualize the computation process of this task, the following \texttt{MRtask} methods were identified for the instrumentation:
\begin{itemize}
	\item \texttt{dfork} - this method is called at the initiator node and starts the computation of the whole task.
	\item \texttt{getResult} - this method is called at the initiator and blocks until the distributed computation finishes.
	\item \texttt{setupLocal0} - this method splits the task, creates sub-tasks for child nodes, sends the sub-tasks to the target child nodes and submits the local task.
	\item \texttt{map} - the map operation. This method needs to be implemented explicitly by the user. 
	\item \texttt{reduce2} - the reduce operation. This method needs to be implemented explicitly by the user. 
	\item \texttt{compute2} - this method controls the computation and internally calls the \texttt{map} implementation.
	\item \texttt{onComplete} - this method waits for the sub-tasks to finish and then calls the \texttt{reduce2} operation on them.
\end{itemize}

We are also interested in how long the remote computation lasts on child nodes. For this reason, we need to instrument the following two methods on the \texttt{RPC} class:
\begin{itemize}
	\item \texttt{call} - this method is called when the remote computation has been submitted.
	\item \texttt{response} - this method is called when the remote computation has been finished and the child node is signaling that its work is done.
\end{itemize}

\section{Building the Core Server and Native Agent}
In order to be able to extend the core instrumentation library, we need to build it first. This won't be necessary once the core instrumentation server is published to some online repository of JAR packages\footnote{For example, Maven Central Repository.}. The native agent also needs to be built on the platform where H\textsubscript{2}O will be running. 

Please see the Attachment 3 for the information on how to build the project from sources or how to run this example from the prepared Docker machine. 

\section{Extending the Core Instrumentation Server}
In order to capture the relationships between tasks and their computations, we need to instrument the methods mentioned in the previous section. It is important to always find a good pair of methods, which open and close a single span. In the case of H\textsubscript{2}O, the following calls have been identified to form good spans. The pairs are ordered from the top level spans encapsulating the whole computation to the local spans encapsulating single mapping or reduce operations.

\begin{enumerate}
	\item \texttt{dfork} - \texttt{getResult}. This pair forms the main span, which encapsulates the complete computation of a single \texttt{MRTask}. This is possible since the \texttt{dfork} method is only called on the node where the computation starts and the same holds for the \texttt{getResult} method after the computation finished. The span is opened when the first method is entered and closed when the latter method is left.
		
	\item \texttt{setupLocal0} - \texttt{onCompletion}. This pair forms the span representing the complete computation on a single node. It encapsulates the local work as well as the waiting for the remote work to complete. This span is opened when the first method is entered and closed when the latter method is left. This span is well-defined since the \texttt{onCompletion} method is called for each task and represents that the work has been finished in all children tasks.
	
	\item \texttt{call} - \texttt{response}. This pair forms the span used to track only the remote computation. It encapsulates the remote computation and all following sub-tasks created by the remote task. This span is opened when the first method is entered and closed when the second method is left.
	
	\item \texttt{compute2} - \texttt{onCompletion}. This pair forms the span used to encapsulate the local computation on a local node. It encapsulates all the cases of local work. In the case of multiple chunks exist for the given local task on the same node, this method is recursively called on split tasks until we have tasks representing a single chunk. The calls are also recursively confirmed by the \texttt{onCompletion} call. In multi-chunk case, only one child task is submitted for the execution into F/J thread. The second task is executed immediately in the same thread. This way it is ensured that we can reuse the existing thread as much as possible. This span is opened when the first method is entered and closed when the latter method is left.
	
	\item \texttt{map} - \texttt{map}. This pair represents the single mapping operation. This span is entered when the \texttt{map} method is entered and closed when the \texttt{map} method is left. Therefore, this span lasts only for the duration of the \texttt{map} method call.
	
	\item \texttt{reduce2} - \texttt{reduce2}. This pair represents the single reduce operation. This span is entered when the \texttt{reduce2} method is entered and closed when the \texttt{reduce2} method is left. Therefore, this span lasts only for the duration of the \texttt{reduce2} method call.
\end{enumerate}

For all of the pairs above, the Advice API is used for the instrumentation because it's sufficient in our case to be able to capture just method enter and exit events. In case we would like to perform more complex instrumentation, we could use the Interceptors API, which is described in the Byte Buddy documentation and briefly also in Section \ref{sec:byte_buddy}.

Instrumenting these methods is a technical task and the code can be seen in the corresponding example available in the Attachment 1. The trace context information is always attached to the task transferred between the nodes. The trace context is initially created during the \texttt{dfork} method call since it's the main entry point of the computation. When instrumenting the \texttt{compute2} and \texttt{setupLocal0} methods, the deep copy of the trace context is attached to the transferred task because the closing \texttt{onComplete} method is usually called from a different thread and this ensures there are no collisions caused by accessing the same trace context from multiple threads.

Also, when instrumenting a few of the mentioned methods, we need to ensure that correct pairs of spans are created and closed. For this purpose, we use flags available on the \texttt{Span} class. Flags allow us to attach additional information to the trace context, which may be used when closing the span. This is, for example, useful in cases when a method is used to close several spans at the same time and we need to properly distinguish between the spans to close the correct one.

Once all the advice or interceptors are created for each of the mentioned methods above, we need to create the transformers. The transformer defines on which method from the code of the application which advice or interceptor is applied. For this purpose, Distrace provides several helper methods allowing us to create transformers in a very concise way. For example, the transformer for \texttt{RPC.class} defining the instrumentation of the \texttt{call} and \texttt{response} methods looks like:

\begin{lstlisting}[language=Java]
new BaseTransformer() {
  @Override
  public DynamicType.Builder<?> 	defineTransformation(
    DynamicType.Builder<?> builder) {

    // Method to instrument
    Method call = ReflectionUtils.getMethod(RPC.class, "call");

    // Apply the advice on the method
    return builder
        .visit(Advice.to(RPCAdvices.call.class).on(is(call)))
        .visit(Advice.to(RPCAdvices.response.class).on(named("response")));
    }
}
\end{lstlisting}
The \texttt{RPCAdvices.call} and \texttt{RPCAdvices.response} static classes are the advice, which define the instrumentation.

Once the transformers for each method to be instrumented are created, we need to associate the transformers with the classes from the application on which they should operate. We also need to create a main entry point of the extended instrumentation server. This is demonstrated on the following code snippet:

\begin{lstlisting}[language=Java]
public static void main(String args[]) {
  new Instrumentor().start(args, new MainAgentBuilder() {
    @Override
    public AgentBuilder createAgent(BaseAgentBuilder builder, 
                                      String pathToHelperClasses) {
	 		 
	 		 return builder
		 		.type(isSubTypeOf(MRTask.class))
				.transform(mrTaskTransformer)
				.type(is(RPC.class))
				.transform(rpcTransformer)
	         }
	  });
}			 
\end{lstlisting}
In this code, we associate classes containing the methods to be instrumented to the transformers handling the instrumentation of those methods. We are also starting the instrumentation server using the \texttt{start} method provided by the core instrumentation server with the \texttt{MainAgentBuilder} instance as the argument. This instance is later used as a dispatcher of the instrumentation of the whole application. Later, when starting the application with the native agent attached and configuring the agent, we need to explicitly specify the class, which is used as the main entry point of the extended instrumentation server. This is exactly the class containing the \texttt{main} method with the content above.

At this step, we need to build the extended instrumentation server. This server has two dependencies: the core instrumentation server and also the H\textsubscript{2}O application sources. The first dependency is obvious as we are using the API defined in the core server library. The second library is required since we are using several application classes when defining the instrumentation points. This may not be necessary when instrumenting different applications since we can identify a class to be instrumented, for example, by its name using the following element matcher \texttt{named("fully.qualified.class.name)} instead of \texttt{is(Example.class)}. However, the second option gives us more freedom when defining the instrumentation points and has also a performance benefit. In this case, the application classes are already located on the server and when they are requested to be instrumented, the  bytecode for each of these classes doesn't have to be sent from the native agent. 

After this step, we should have two artifacts - the native agent library built for our platform from the previous steps and also the extended instrumentation server JAR file from this step.
\section{Configuring and Running the Application}
For the purposes of this example, we start the cluster of three H\textsubscript{2}O nodes. Two of them are regular nodes and the last node contains the main method in which the \textbf{SumMRTask} is executed. This task is used to sum up range of numbers in distributed manner.

An arbitrary H\textsubscript{2}O node can be started as: \newline \texttt{java -jar h2o.jar -name cluster\_name}.\newline When operating on the network with multi-cast communication enabled, multiple nodes started with the same cluster name automatically form a single cluster. If we want to start the application with the monitoring agent attached, we can use the \texttt{-agentlib} java option. Any H\textsubscript{2}O node can be started with the monitoring feature enabled by calling the following command: \newline
\texttt{java -agentpath:"\$NATIVE\_AGENT\_LIB\_PATH=\$AGENT\_ARGS" -jar h2o.jar  \newline -name cluster\_name}

The \texttt{\$NATIVE\_AGENT\_LIB\_PATH} shell variable needs to point to the location of the native agent library and \texttt{\$AGENT\_ARGS} shell variable may contain any arguments passed to the native agent. The arguments are in the \texttt{key=value} format and are separated by the semicolon. The complete list of available arguments is in the Attachment 4.

In the case of this example, we let the native agent start the instrumentation server locally for each node automatically. Therefore, the inter-process communication is used and we don't need to configure it explicitly. Only two arguments need to be specified to the native agent - the path to the instrumentation server and the fully qualified name of the main entry class of the instrumentation server. Therefore, the full command starting a single H\textsubscript{2}O node with the monitoring agent enabled may be: \newline
\texttt{java -agentpath:"/home/agent.so=instrumentor\_server\_jar=\newline/home/instrumentor.jar;instrumentor\_main\_class=main.entry.pint" \newline-jar h2o.jar -name cluster\_name}

In order to start the cluster of size three, we need to call this command three times. Twice with the regular H\textsubscript{2}O node and once with the H\textsubscript{2}O node containing the execution of the \texttt{SumMRTask}. It is also important to start the Zipkin user interface to which the results are published. The user interface server may be started as: \texttt{java -jar zipkin.jar}\footnote{The Zipkin Jar file may can downloaded from \url{https://github.com/openzipkin/zipkin} or is available at the attached CD.}.
\section{The Results}
Once all three nodes have been started, the computation starts and the results based on our instrumentation are shown directly in the Zipkin user interface. By default, the user interface is available at port 9411.

We need to click on the \textit{Find Traces} button to show all traces, which satisfy our search conditions. We should see in the output a single trace and once we click on it, we should see a similar result to the one in Figure \ref{h2o_zipkin_output}. This figure shows just a portion of the whole trace but contains the important observed information.
	\begin{figure}
		\centering
		\includegraphics[scale=0.4]{h2o_zipkin_output.png}
		\caption{Example trace from the distributed computation on H\textsubscript{2}O platform.}
		\label{h2o_zipkin_output}
	\end{figure}

All the operations and their timing can be seen on the output and we can see how long each operation lasted and when it started. The spans are also organized hierarchically as they were created. We can see the main span encapsulating the whole computation, the spans for computation on each node and also spans encapsulating the remote computations. The local \texttt{map} and \texttt{reduce} calls are displayed as well. It is, for example, interesting to see how long it took the platform to perform the \texttt{reduce} operation after the \texttt{map} operation has been called.